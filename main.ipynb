{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-14a3cdabd43e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-14a3cdabd43e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ===========================================\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "===========================================\n",
    "=                                         =\n",
    "=      EXTRACTING DATA FROM CORPUS        =\n",
    "=                                         =\n",
    "==========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import of modules\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL NO. OF FILES  500 \n",
      "\n",
      "RUNNING ON  100  FILES\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#listing corpus dir and choosing number of files\n",
    "files = os.listdir('brown/')\n",
    "n_sample_files = 100\n",
    "\n",
    "print('TOTAL NO. OF FILES ', len(files), '\\n')\n",
    "print('RUNNING ON ', n_sample_files, ' FILES\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS SIZE 16638 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reading files from corpus and creating list with contents\n",
    "raw_corpus = ''\n",
    "\n",
    "for file in files[0:n_sample_files]:\n",
    "    with open('brown/' + file) as f:\n",
    "        raw_corpus = raw_corpus + '\\n' + f.read()\n",
    "\n",
    "corpus = raw_corpus.split('\\n')\n",
    "print('CORPUS SIZE', len(corpus), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He/pps arraigned/vbd the/at citizens/nns in/in language/nn of/in so/ql little/ap courtesy/nn that/cs they/ppss had/hvd to/to respond/vb with/in ,/, at/in the/at least/ap ,/, resentment/nn ./.\n"
     ]
    }
   ],
   "source": [
    "# initalizing variables\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "words = []\n",
    "tags = []\n",
    "\n",
    "with_slash = False\n",
    "n_omitted = 0\n",
    "\n",
    "print(corpus[70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract words and tags and separate them and remove omitted words\n",
    "for line in corpus:\n",
    "    # for every file with content\n",
    "    if(len(line)>0):\n",
    "        tempX = []\n",
    "        tempY = []\n",
    "        for word in line.split():\n",
    "            try:\n",
    "                # if word has '/', first part is word, second is tag\n",
    "                w, tag = word.split('/')\n",
    "            except:\n",
    "                # with_slash = True\n",
    "                # else word generaly containing \"/\" \n",
    "                n_omitted = n_omitted + 1\n",
    "                #print(word)\n",
    "                #print(line)\n",
    "                #print(\"\\n\")\n",
    "                break\n",
    "\n",
    "            w = w.lower()\n",
    "\n",
    "            # save word and tag\n",
    "            words.append(w)\n",
    "            tags.append(tag)\n",
    "            \n",
    "            # add each word and tag\n",
    "            tempX.append(w)\n",
    "            tempY.append(tag)\n",
    "        \n",
    "        # add words and tags from sentence\n",
    "        X_train.append(tempX)\n",
    "        Y_train.append(tempY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMITTED sentences:  46 \n",
      "\n",
      "TOTAL NO OF SAMPLES:  11014 \n",
      "\n",
      "sample X_train:  ['opinion', 'is', 'less', 'individual', 'or', 'runs', 'more', 'into', 'masses', ',', 'and', 'often', 'rules', 'with', 'a', 'rod', 'of', 'iron', \"''\", '.'] \n",
      "\n",
      "sample Y_train:  ['nn', 'bez', 'ql', 'jj', 'cc', 'vbz', 'rbr', 'in', 'nns', ',', 'cc', 'rb', 'vbz', 'in', 'at', 'nn', 'in', 'nn', \"''\", '.'] \n",
      "\n",
      "VOCAB SIZE:  20115\n",
      "TOTAL TAGS:  332\n"
     ]
    }
   ],
   "source": [
    "print('OMITTED sentences: ', n_omitted, '\\n')\n",
    "print('TOTAL NO OF SAMPLES: ', len(X_train), '\\n')\n",
    "\n",
    "print('sample X_train: ', X_train[42], '\\n')\n",
    "print('sample Y_train: ', Y_train[42], '\\n')\n",
    "\n",
    "# removing duplicates\n",
    "words = set(words)\n",
    "tags = set(tags)\n",
    "\n",
    "print('VOCAB SIZE: ', len(words))\n",
    "print('TOTAL TAGS: ', len(tags))\n",
    "\n",
    "# number of words and tags should be the same\n",
    "assert len(X_train) == len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample X_train_numberised:  [6040, 11071, 4038, 8160, 14716, 5223, 5168, 10922, 2295, 541, 4999, 16518, 1903, 9470, 1720, 2348, 10244, 13812, 8508, 17707] \n",
      "\n",
      "sample Y_train_numberised:  [133, 324, 140, 166, 249, 256, 184, 174, 98, 243, 249, 182, 256, 174, 26, 133, 174, 133, 309, 132] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# enumerate words and tags in sets\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    word2int[word] = i+1\n",
    "    int2word[i+1] = word\n",
    "\n",
    "tag2int = {}\n",
    "int2tag = {}\n",
    "\n",
    "for i, tag in enumerate(tags):\n",
    "    tag2int[tag] = i+1\n",
    "    int2tag[i+1] = tag\n",
    "    \n",
    "X_train_numberised = []\n",
    "Y_train_numberised = []\n",
    "\n",
    "# save ordinal numbers of words and tags in sentences\n",
    "for sentence in X_train:\n",
    "    tempX = []\n",
    "    for word in sentence:\n",
    "        tempX.append(word2int[word])\n",
    "    X_train_numberised.append(tempX)\n",
    "    \n",
    "for tags in Y_train:\n",
    "    tempY = []\n",
    "    for tag in tags:\n",
    "        tempY.append(tag2int[tag])\n",
    "    Y_train_numberised.append(tempY)\n",
    "\n",
    "print('sample X_train_numberised: ', X_train_numberised[42], '\\n')\n",
    "print('sample Y_train_numberised: ', Y_train_numberised[42], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([12205, 541, 16050, 10453, 2174, 19127, 7920, 9612, 541, 2074, 19752, 11071, 12900, 19127, 19654, 8920, 4420, 15483, 18011, 1720, 18318, 16676, 19127, 1539, 19127, 15455, 11665, 2074, 11737, 17707])\n",
      " list([14478, 14643, 6964, 6006, 10453, 1062, 14868, 10244, 7885, 18045, 4023, 16050, 14478, 13486, 12042, 936, 18607, 10224, 14478, 10549, 11249, 4023, 4999, 7559, 12042, 7631, 8503, 1720, 1871, 541, 8974, 10224, 15666, 14716, 10224, 499, 17707])\n",
      " list([3873, 9608, 11418, 541, 10453, 9331, 10068, 10244, 14478, 13336, 541, 16370, 15606, 336, 19127, 14478, 16469, 10224, 17437, 541, 5411, 8920, 7559, 4569, 8503, 4774, 3525, 14716, 18828, 4999, 8920, 10224, 10132, 17786, 15871, 14052, 19127, 10839, 4013, 6577, 13948, 5866, 17774, 17707])\n",
      " ... list([14478, 2351, 9858, 9470, 15073, 4999, 15684, 17707])\n",
      " list([12738, 12993, 11071, 1720, 7069, 18322, 541, 4999, 3458, 2524, 541, 541, 14478, 8666, 10244, 14041, 541, 14202, 4038, 11628, 17707])\n",
      " list([10244, 4386, 12663, 4999, 3458, 3310, 541, 2074, 11071, 18107, 19127, 4847, 17707])]\n",
      "Saved as pickle file\n"
     ]
    }
   ],
   "source": [
    "# create arrays from lists\n",
    "X_train_numberised = np.asarray(X_train_numberised)\n",
    "Y_train_numberised = np.asarray(Y_train_numberised)\n",
    "\n",
    "# collect data to save in picle files\n",
    "pickle_files = [X_train_numberised, Y_train_numberised, word2int, int2word, tag2int, int2tag]\n",
    "\n",
    "# create directory if it doesn't exist\n",
    "if not os.path.exists('PickledData/'):\n",
    "    print('MAKING DIRECTORY PickledData/ to save pickled glove file')\n",
    "    os.makedirs('PickledData/')\n",
    "\n",
    "# open file and write \n",
    "with open('PickledData/data.pkl', 'wb') as f:\n",
    "    pickle.dump(pickle_files, f)\n",
    "\n",
    "print('Saved as pickle file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "===========================================\n",
    "=                                         =\n",
    "=      CONVERT TXT TO PICKLE FILE         =\n",
    "=                                         =\n",
    "==========================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
